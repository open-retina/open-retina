---
title: How to cite
---

Training retinal models using `openretina` relies on the neural recordings provided by experimental labs, and on the model architectures that we integrated into our package.
When using our software for scientific purposes, we encourage you to cite the original data sources the model you used was trained on, the original papers that developed the model architecture you are using, and if you are kind, also the repository and paper of our package.

To make this easy, below we provide a starting point of the most important references. Contact us or raise a pull request in case we missed something!

## Openretina
- [BioRxiv Paper](https://www.biorxiv.org/content/10.1101/2025.03.07.642012v1)
- [Git Repository Citation](https://github.com/open-retina/open-retina/blob/main/CITATION.cff)

## Data sources
When using data sources through openretina, we recommend to both cite the paper that originally described and analysed the data and reference or cite the data source location:

* hoefling\_2024: Originally published by Höfling et al. (2024), eLife
  * Paper: [A chromatic feature detector in the retina signals visual context changes](https://doi.org/10.7554/eLife.86860).
  * Dataset originally deposited at: https://gin.g-node.org/eulerlab/rgc-natstim
* karamanlis\_2024: Originally published by Karamanlis et al. (2024), Nature
  * Paper: [Nonlinear receptive fields evoke redundant retinal coding of natural scenes](https://doi.org/10.1038/s41586-024-08212-3)
  * Dataset: Karamanlis D, Gollisch T (2023) Dataset - Marmoset and mouse retinal ganglion cell responses to natural stimuli and supporting data. G-Node. https://doi.org/10.12751/g-node.ejk8kx
* maheswaranathan\_2023: Originally published by Maheswaranathan et al. (2023), Neuron
  * Paper: [Interpreting the retinal neural code for natural scenes: From computations to neurons](https://doi.org/10.1016/j.neuron.2023.06.007)
  * Dataset: Maheswaranathan, N., McIntosh, L., Tanaka, H., Grant, S., Kastner, D., Melander, J., Nayebi, A., Brezovec, L., Wang, J. Ganguli, S. Baccus, S. (2023). Interpreting the retinal neural code for natural scenes: from computations to neurons. Stanford Digital Repository. Available at https://purl.stanford.edu/rk663dm5577
* goldin\_2022: Originally published by Goldin et al. (2022), Nature Communications
  * Paper: [Context-dependent selectivity to natural images in the retina](https://doi.org/10.1038/s41467-022-33242-8)
  * Dataset originally deposited at: https://zenodo.org/records/6868362

## Modelling

In additon to the paper mentioned above, here are some influential papers that train artificial neural networks on recordings from visual brain areas like the retina:

* [Deep learning models of the retinal response to natural scenes](https://proceedings.neurips.cc/paper/2016/file/a1d33d0dfec820b41b54430b50e96b5c-Paper.pdf)
* [Multilayer Recurrent Network Models of Primate Retinal Ganglion Cell Responses](https://openreview.net/pdf?id=HkEI22jeg)
* [Neural system identification for large populations separating “what”and “where” in Advances in Neural Information Processing Systems](https://proceedings.neurips.cc/paper/2017/file/8c249675aea6c3cbd91661bbae767ff1-Paper.pdf)




## Missing information

If you would like to see additional papers references on this page, contact us via a Github [issue](https://github.com/open-retina/open-retina/issues) or directly raise a [pull request](https://github.com/open-retina/open-retina/pulls) for adding the respective information.
