{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to explain how to use a new dataset with `openretina`.\n",
    "\n",
    "For this example, we are going to use data from Maheswaranathan et al. (2023): [Interpreting the retinal neural code for natural scenes: From computations to neurons](https://doi.org/10.1016/j.neuron.2023.06.007) .\n",
    "\n",
    "Along the way, we are also going to address some questions that can arise regarding the process for your own data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import lightning\n",
    "\n",
    "from openretina.data_io.base import MoviesTrainTestSplit, ResponsesTrainTestSplit, compute_data_info\n",
    "from openretina.data_io.base_dataloader import multiple_movies_dataloaders\n",
    "from openretina.data_io.cyclers import LongCycler, ShortCycler\n",
    "from openretina.models.core_readout import CoreReadout\n",
    "from openretina.utils.file_utils import get_cache_directory, get_local_file_path\n",
    "from openretina.utils.h5_handling import load_dataset_from_h5, load_h5_into_dict\n",
    "from openretina.utils.misc import CustomPrettyPrinter\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")  # to display logs in jupyter notebooks\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "pp = CustomPrettyPrinter(indent=4, max_lines=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's set the cache directory for the data and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/baptiste/openretina_cache'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The default directory for downloads will be ~/openretina_cache\n",
    "# To change this, uncomment the following line and change its path\n",
    "# os.environ[\"OPENRETINA_CACHE_DIRECTORY\"] = \"/Data/\"\n",
    "\n",
    "# You can then check if that directory has been correctly set by running:\n",
    "get_cache_directory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now download the data from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 13:04:21,543 - INFO - Fetching file list for open-retina/open-retina...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 13:04:21,765 - INFO - Found extracted folder at /home/baptiste/openretina_cache/baccus_lab/maheswaranathan_2023/neural_code_data\n"
     ]
    }
   ],
   "source": [
    "data_path = get_local_file_path(\n",
    "    \"https://huggingface.co/datasets/open-retina/open-retina/blob/main/baccus_lab/maheswaranathan_2023/neural_code_data.zip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's inspect the structure of this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15-10-07  15-11-21a  15-11-21b\n"
     ]
    }
   ],
   "source": [
    "!ls $data_path/ganglion_cell_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the ganglion cell data is structured by sessions. We are going to pick session `15-10-07` to use throughout the examples in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naturalscene.h5  whitenoise.h5\n"
     ]
    }
   ],
   "source": [
    "!ls $data_path/ganglion_cell_data/15-10-07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside each session we have files for two different type of stimuli.\n",
    "\n",
    "Let's load the file dealing with whitenoise and inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4376c7f1f1d449c2a1750996f3181ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading HDF5 file contents:   0%|          | 0/30 [00:00<?, ?item/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'spikes': {   'cell01': numpy.ndarray(shape=(43927,)),\n",
      "                  'cell02': numpy.ndarray(shape=(11819,)),\n",
      "                  'cell03': numpy.ndarray(shape=(12423,)),\n",
      "                  'cell04': numpy.ndarray(shape=(37662,)),\n",
      "                  'cell05': numpy.ndarray(shape=(10976,)),\n",
      "                  'cell06': numpy.ndarray(shape=(11654,)),\n",
      "                  'cell07': numpy.ndarray(shape=(17792,)),\n",
      "                  'cell08': numpy.ndarray(shape=(4566,)),\n",
      "                  'cell09': numpy.ndarray(shape=(36307,))},\n",
      "    'test': {   'repeats': {   'cell01': numpy.ndarray(shape=(6, 5997)),\n",
      "                               'cell02': numpy.ndarray(shape=(6, 5997)),\n",
      "                               'cell03': numpy.ndarray(shape=(6, 5997)),\n",
      "                               'cell04': numpy.ndarray(shape=(6, 5997)),\n",
      "                               'cell05': numpy.ndarray(shape=(6, 5997)),\n",
      "                               'cell06': numpy.ndarray(shape=(6, 5997)),\n",
      "                               'cell07': numpy.ndarray(shape=(6, 5997)),\n",
      "                               'cell08': numpy.ndarray(shape=(6, 5997)),\n",
      "                               'cell09': numpy.ndarray(shape=(6, 5997))},\n",
      "                'response': {   'binned': numpy.ndarray(shape=(9, 5997)),\n",
      "                                'firing_rate_10ms': numpy.ndarray(shape=(9, 5997)),\n",
      "                                'firing_rate_20ms': numpy.ndarray(shape=(9, 5997)),\n",
      "                                'firing_rate_5ms': numpy.ndarray(shape=(9, 5997))},\n",
      "                'stimulus': numpy.ndarray(shape=(5996, 50, 50)),\n",
      "                'time': numpy.ndarray(shape=(5996,))},\n",
      "    'train': {   'response': {   'binned': numpy.ndarray(shape=(9, 359802)),\n",
      "                                 'firing_rate_10ms': numpy.ndarray(shape=(9, 359802)),\n",
      "                                 'firing_rate_20ms': numpy.ndarray(shape=(9, 359802)),\n",
      "                                 'firing_rate_5ms': numpy.ndarray(shape=(9, 359802))},\n",
      "                 'stimulus': numpy.ndarray(shape=(359802, 50, 50)),\n",
      "                 'time': numpy.ndarray(shape=(359802,))}}\n"
     ]
    }
   ],
   "source": [
    "whitenoise_file = load_h5_into_dict(os.path.join(data_path, \"ganglion_cell_data\", \"15-10-07\", \"whitenoise.h5\"))\n",
    "\n",
    "pp.pprint(whitenoise_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that at the first level of the `.h5` hierarchy the data is split into `train`, `test` and `spikes`. \n",
    "\n",
    "`spikes` will contain the spike times for each neuron, which we can ignore. \n",
    "\n",
    "`train` and `test` are structured similarly: they both contain numpy arrays for the stimulus, time (mapping to the spike indices in `spikes`) and the response. The latter is saved with different binnings (by choosing a different bin width in time, there are more ways to group a sequence of spike times into a firing rate representation). \n",
    "\n",
    "We can see that the stimulus and the response arrays share the time dimensions. These are the data we are interested in for model fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how we can load this to use with `openretina`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data\n",
    "\n",
    "What we need is the matching stimulus and response pairs for training and testing. We will then need to feed them inside the two classes that handle their data, respectively `ResponsesTrainTestSplit` and `MoviesTrainTestSplit`.\n",
    "\n",
    "Let's briefly print the classes help information, so we can see which arguments they expect:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInit signature:\u001b[39m\n",
      "MoviesTrainTestSplit(\n",
      "    train: jaxtyping.Float[ndarray, \u001b[33m'channels train_time height width'\u001b[39m],\n",
      "    test_dict: dict = <factory>,\n",
      "    test: dataclasses.InitVar[jaxtyping.Float[ndarray, \u001b[33m'channels test_time height width'\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    stim_id: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    random_sequences: Optional[numpy.ndarray] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    norm_mean: Optional[float] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    norm_std: Optional[float] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      ") -> \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mDocstring:\u001b[39m      MoviesTrainTestSplit(train: jaxtyping.Float[ndarray, 'channels train_time height width'], test_dict: dict = <factory>, test: dataclasses.InitVar[jaxtyping.Float[ndarray, 'channels test_time height width'] | None] = None, stim_id: Optional[str] = None, random_sequences: Optional[numpy.ndarray] = None, norm_mean: Optional[float] = None, norm_std: Optional[float] = None)\n",
      "\u001b[31mFile:\u001b[39m           ~/Documents/LabPipelines/open-retina/openretina/data_io/base.py\n",
      "\u001b[31mType:\u001b[39m           type\n",
      "\u001b[31mSubclasses:\u001b[39m     "
     ]
    }
   ],
   "source": [
    "MoviesTrainTestSplit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInit signature:\u001b[39m\n",
      "ResponsesTrainTestSplit(\n",
      "    train: jaxtyping.Float[ndarray, \u001b[33m'neurons train_time'\u001b[39m],\n",
      "    test_dict: dict = <factory>,\n",
      "    test: dataclasses.InitVar[jaxtyping.Float[ndarray, \u001b[33m'neurons test_time'\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    test_by_trial: jaxtyping.Float[ndarray, \u001b[33m'trials neurons test_time'\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    stim_id: str | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    session_kwargs: dict[str, typing.Any] = <factory>,\n",
      ") -> \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mDocstring:\u001b[39m      ResponsesTrainTestSplit(train: jaxtyping.Float[ndarray, 'neurons train_time'], test_dict: dict = <factory>, test: dataclasses.InitVar[jaxtyping.Float[ndarray, 'neurons test_time'] | None] = None, test_by_trial: jaxtyping.Float[ndarray, 'trials neurons test_time'] | None = None, stim_id: str | None = None, session_kwargs: dict[str, typing.Any] = <factory>)\n",
      "\u001b[31mFile:\u001b[39m           ~/Documents/LabPipelines/open-retina/openretina/data_io/base.py\n",
      "\u001b[31mType:\u001b[39m           type\n",
      "\u001b[31mSubclasses:\u001b[39m     "
     ]
    }
   ],
   "source": [
    "ResponsesTrainTestSplit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now start importing the data that we will feed into these classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train stimulus shape: (359802, 50, 50)\n",
      "Train response shape: (9, 359802)\n",
      "Test stimulus shape: (5996, 50, 50)\n",
      "Test response shape: (9, 5997)\n"
     ]
    }
   ],
   "source": [
    "test_stimulus = load_dataset_from_h5(\n",
    "    os.path.join(data_path, \"ganglion_cell_data\", \"15-10-07\", \"whitenoise.h5\"), \"test/stimulus\"\n",
    ")\n",
    "test_response = load_dataset_from_h5(\n",
    "    os.path.join(data_path, \"ganglion_cell_data\", \"15-10-07\", \"whitenoise.h5\"), \"test/response/firing_rate_20ms\"\n",
    ")\n",
    "\n",
    "train_stimulus = load_dataset_from_h5(\n",
    "    os.path.join(data_path, \"ganglion_cell_data\", \"15-10-07\", \"whitenoise.h5\"), \"train/stimulus\"\n",
    ")\n",
    "train_response = load_dataset_from_h5(\n",
    "    os.path.join(data_path, \"ganglion_cell_data\", \"15-10-07\", \"whitenoise.h5\"), \"train/response/firing_rate_20ms\"\n",
    ")\n",
    "\n",
    "print(f\"Train stimulus shape: {train_stimulus.shape}\")\n",
    "print(f\"Train response shape: {train_response.shape}\")\n",
    "\n",
    "print(f\"Test stimulus shape: {test_stimulus.shape}\")\n",
    "print(f\"Test response shape: {test_response.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the shapes of the arrays we just imported, we need to make some small adjustments to match the assumptions that the classes within `openretina` make. \n",
    "\n",
    "- The stimulus needs to be 4-dimensional, with shape `color_channels x time x height x width`: in this case the channel dimension is missing.\n",
    "- The responses need to have shape `n_neurons x time`: this is already the case here.\n",
    "- The stimuli and responses time dimension should match exactly: in this case the test response seems to have one extra time bin, which we are simply going to cut in this case.\n",
    "\n",
    "Let's do all of this here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train stimulus shape: (1, 359802, 50, 50)\n",
      "Train response shape: (9, 359802)\n",
      "Test stimulus shape: (1, 5996, 50, 50)\n",
      "Test response shape: (9, 5996)\n"
     ]
    }
   ],
   "source": [
    "test_stimulus = test_stimulus.reshape(1, -1, test_stimulus.shape[1], test_stimulus.shape[2])\n",
    "train_stimulus = train_stimulus.reshape(1, -1, train_stimulus.shape[1], train_stimulus.shape[2])\n",
    "\n",
    "test_response = test_response[:, :-1]\n",
    "\n",
    "print(f\"Train stimulus shape: {train_stimulus.shape}\")\n",
    "print(f\"Train response shape: {train_response.shape}\")\n",
    "\n",
    "print(f\"Test stimulus shape: {test_stimulus.shape}\")\n",
    "print(f\"Test response shape: {test_response.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before finally initialising our target functions, we should normalise the stimuli (and optionally the responses). This is mostly done to stabilise training, as too wide of an input data range can lead to exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stim_mean = train_stimulus.mean()\n",
    "train_stim_std = train_stimulus.std()\n",
    "\n",
    "norm_train_stimulus = (train_stimulus - train_stim_mean) / train_stim_std\n",
    "norm_test_stimulus = (test_stimulus - train_stim_mean) / train_stim_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can initialise the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_stimulus = MoviesTrainTestSplit(\n",
    "    train=norm_train_stimulus,\n",
    "    test=norm_test_stimulus,\n",
    "    stim_id=\"whitenoise\",\n",
    "    norm_mean=train_stim_mean,\n",
    "    norm_std=train_stim_std,\n",
    ")\n",
    "\n",
    "single_response = ResponsesTrainTestSplit(\n",
    "    train=train_response,\n",
    "    test=test_response,\n",
    "    stim_id=\"whitenoise\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: How to do this step with your data?\n",
    "\n",
    "What matters for how the pipeline is configured within `openretina` is that you can import your data in a way that stimuli and responses for each session have the same sampling frequency, and that you can then end up with two numpy arrays, one for the stimuli and one for the responses, at the same sampling rate (i.e. having the exact same length in the time dimension).\n",
    "\n",
    "This might require some resampling if it is not the case already, and the workflow will vary depending on how your data is exported. This decision and implementation are the responsibility of the user.\n",
    "\n",
    "### Q: What if I do not have a train and a test split in my data?\n",
    "\n",
    "The train/test split is completely arbitrary, but it is sometimes a direct consequence of certain experimental design choices. For example, test stimuli usually have been repeated multiple times, so that an average response can be computed, along with different estimates of SNR or response reliability. Training stimuli on the other hand tend to have a lower number of repeats, often only 1.\n",
    "\n",
    "If all your stimuli have multiple repeats by design and no clear train/test separation, you can then decide which parts you want to use for training and which for testing, for example by doing a 80% / 20% split. It is recommended to use the **average test trace** across repetitions for testing. On the contrary, during training, it can be beneficial to introduce some noise and it is recommended use the single repeats (this will also lead to having more training data).\n",
    "\n",
    "If your data has no clear trial/repetition structure, and you only have 1 repeat per stimulus, you can similarly arbitrarly decide how to split your data, and how much to leave for testing. What you can expect in this case, however, is to have lower test performance compared to what you would get if your test responses were actually collected across multiple trials. The reason for this is simply that having more trials averages out noise, which otherwise is treated as ground-truth signal when computing test performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to initialise a dataloader with the stimuli and responses we extracted. Note that dataloader functions within `openretina` assume that you input a **dictionary** of stimuli and responses, where keys are session names and values are instances of `ResponsesTrainTestSplit` and `MoviesTrainTestSplit` classes we just created. \n",
    "\n",
    "We make this assumption to accommodate multiple experimental sessions for training, which is the usual case. \n",
    "If you indeed have data from multiple sessions, you have two options moving forward.\n",
    "\n",
    "1. Manually repeat what we have done above for all sessions\n",
    "2. **Recommended**: code up your personal *data_io* functions / modules, one for the stimuli and one for the responses. The output of these functions should be two dictionaries that share the keys (i.e. the session names), and have as values the different `ResponsesTrainTestSplit` and `MoviesTrainTestSplit` objects. If you take this route, you can insert such functions inside `openretina.data_io.your_dataset_name` and if you feel like sharing, submit a PR to us such that we can include your dataset in the repository! To see a worked example, check out how we coded up the functions to do so for the current dataset at `openretina.data_io.maheswaranathan_2023`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep things simple, here we simply initialise one-item dictionaries for the stimuli and responses we just extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli = {\"15-10-07\": single_stimulus}\n",
    "\n",
    "responses = {\"15-10-07\": single_response}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to feed our matching dictionaries of stimuli and responses to a dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414b451b7ae841f7a6e15cbc59052011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating movie dataloaders:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloaders = multiple_movies_dataloaders(neuron_data_dictionary=responses, movies_dictionary=stimuli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 359802, 50, 50)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimuli['15-10-07'].train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'dict'>,\n",
      "            {   'test': {   '15-10-07': torch.utils.data.DataLoader(Dataset: MovieDataSet with 9 neuron responses to a movie of shape [1, 5996, 50, 50].)},\n",
      "                'train': {   '15-10-07': torch.utils.data.DataLoader(Dataset: MovieDataSet with 9 neuron responses to a movie of shape [1, 358800, 50, 50].)},\n",
      "                'validation': {   '15-10-07': torch.utils.data.DataLoader(Dataset: MovieDataSet with 9 neuron responses to a movie of shape [1, 1000, 50, 50].)}})\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(dataloaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialising a simple model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digital twin models (as are ML models in general) are very much dependent on the data they are trained and evaluated on, even in model architecture. Practically:\n",
    "\n",
    "- The shape of the input stimulus will influence the shape of the convolutional kernels, and is therefore a parameter at model creation\n",
    "- The number of sessions and the neurons in each session will, in turn, influence the structure and number of parameters in the readout networks, and are also parameters at model creation.\n",
    "\n",
    "To get this information from the data and pass it to the model and store it, `openretina` has an utility function, `compute_data_info`, which takes as arguments the same two dictionaries that are fed to the dataloader function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'input_shape': (1, 50, 50),\n",
      "    'movie_norm_dict': {   '15-10-07': {   'norm_mean': np.float64(64.0036423232778),\n",
      "                                           'norm_std': np.float64(63.99999989635505)}},\n",
      "    'n_neurons_dict': {'15-10-07': 9},\n",
      "    'sessions_kwargs': {'15-10-07': {}}}\n"
     ]
    }
   ],
   "source": [
    "data_info = compute_data_info(neuron_data_dictionary=responses, movies_dictionary=stimuli)\n",
    "\n",
    "# Display the data info\n",
    "pp.pprint(data_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialise a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baptiste/Documents/LabPipelines/open-retina/openretina/models/core_readout.py:189: UserWarning: Cutting frames from the core output can lead to unexpected results if the input is not padded.self._cut_first_n_frames=30, input_padding=False. Double check the core output shape.\n",
      "  core = SimpleCoreWrapper(\n",
      "2025-04-29 13:57:12,627 - INFO - in_shape_readout=torch.Size([64, 68, 44, 44])\n"
     ]
    }
   ],
   "source": [
    "model = CoreReadout(\n",
    "    in_shape=(1, 100, 50, 50),  # Note that data_info does not include time, we add a dummy time dimension here.\n",
    "    hidden_channels=[32, 64],\n",
    "    temporal_kernel_sizes=[3, 3],\n",
    "    spatial_kernel_sizes=[7, 7],\n",
    "    n_neurons_dict=data_info[\"n_neurons_dict\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step before initiating training is now to wrap the training, validation and testing dataloaders (which are, in fact, dictionaries of dataloaders) into a `Cycler` object, which is an utility that will go through the data for each session. \n",
    "\n",
    "(Note that we still need to do this in our one-session running example, because `dataloaders[\"train\"]`, `dataloaders[\"validation\"]` and `dataloaders[\"test\"]` will still be dictionaries, in this case of only one item. Feel free to inspect a bit more the `dataloaders` dictionary we created to make sense of this.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = LongCycler(dataloaders[\"train\"])\n",
    "val_loader = ShortCycler(dataloaders[\"validation\"])\n",
    "test_loader = ShortCycler(dataloaders[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will just check whether the trainer works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                        | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | core             | SimpleCoreWrapper           | 106 K  | train\n",
      "1 | readout          | MultiGaussianReadoutWrapper | 621    | train\n",
      "2 | loss             | PoissonLoss3d               | 0      | train\n",
      "3 | correlation_loss | CorrelationLoss3d           | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "106 K     Trainable params\n",
      "0         Non-trainable params\n",
      "106 K     Total params\n",
      "0.428     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc7cb39fbb546498df124029ee62014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493c4e759e6d49f6915c4973fafe60a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = lightning.Trainer(fast_dev_run=True)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to set up training using a training script, either a custom one or using our unified interface. More on that below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using new data with our unified training script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`openretina` comes with a few command line scripts, among which `openretina train`. This calls our training script, which uses `hydra` for config management.\n",
    "\n",
    "A few things are needed to run training on a completely new dataset using our training script:\n",
    "\n",
    "1. Creating data_io and dataloading functions for the new dataset, and placing them in the `openretina/data_io` submodule. Earlier parts of this notebook dealt with this.\n",
    "2. Creating data_io and dataloader configs for the new dataset, and placing them in the appropriate folders in `configs`.\n",
    "3. Creating an \"outer\" config, to place as a direct children in the `configs` folder.\n",
    "\n",
    "Let's go through these step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creating a custom sub-module for the new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing something similar to what we did in this notebook, you would need to code up functions for the stimuli and for the responses that create two dictionaries that share the keys (i.e. the session names), and have as values `ResponsesTrainTestSplit` and `MoviesTrainTestSplit` objects. \n",
    "\n",
    "Extending the example in this notebook, we already provide such function for the `maheswaranathan_2023` dataset under `openretina.data_io.maheswaranathan_2023`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating config files for data_io and dataloading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once such dataloading functions are in place, we need to make sure they are used correctly in the training script.\n",
    "This is how dataloading happens in the training script:\n",
    "\n",
    "```{python}\n",
    "movies_dict = hydra.utils.call(cfg.data_io.stimuli)\n",
    "neuron_data_dict = hydra.utils.call(cfg.data_io.responses)\n",
    "\n",
    "dataloaders = hydra.utils.instantiate(\n",
    "    cfg.dataloader,\n",
    "    neuron_data_dictionary=neuron_data_dict,\n",
    "    movies_dictionary=movies_dict,\n",
    ")\n",
    "```\n",
    "\n",
    "Let's break this down in the case for the stimuli.\n",
    "\n",
    "`hydra.utils.call` is calling a function which is found in the config at `data_io.stimuli`. \n",
    "In the main configuration files folder (called `configs`), we have different subfolders for different possibilities of configuration options. In the `data_io` folder we have different `YAML` files dealing with the data_io functions. There, we created a file called `maheswaranathan_2023.yaml` which looks like this:\n",
    "\n",
    "```{yaml}\n",
    "stimuli:\n",
    "  _target_: openretina.data_io.maheswaranathan_2023.stimuli.load_all_stimuli\n",
    "  _convert_: object\n",
    "  base_data_path: ${data.data_dir}\n",
    "  stim_type: \"naturalscene\"\n",
    "  normalize_stimuli: true\n",
    "\n",
    "\n",
    "responses:\n",
    "  _target_: openretina.data_io.maheswaranathan_2023.responses.load_all_responses\n",
    "  _convert_: object\n",
    "  base_data_path: ${data.data_dir}\n",
    "  stim_type: \"naturalscene\"\n",
    "  response_type: \"firing_rate_20ms\"\n",
    "  fr_normalization: 1.0\n",
    "```\n",
    "\n",
    "When we call hydra.utils.call(cfg.data_io.stimuli), Hydra looks up the stimuli key in our configuration and finds that it specifies a function to call:\n",
    "- `_target_`: Specifies the fully qualified function path that should be called, in this case, `openretina.data_io.maheswaranathan_2023.stimuli.load_all_stimuli`.\n",
    "- `_convert_`: Ensures that the output of the function is returned as an object rather than a dictionary.\n",
    "- The rest are arguments specific to the function that we coded up.\n",
    "\n",
    "Importantly then, when adding the configuration for a new dataset, the user should specify in a a new config file under `data_io` which function should be called and with which parameters such that they will return the dictionary of keys to `ResponsesTrainTestSplit` and `MoviesTrainTestSplit` objects.\n",
    "\n",
    "The same holds for dataloading.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating an \"outer\" config."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once `data_io` functions are coded up, and `data_io` configs are created, these will need to be referenced in an \"outer\" config file which orchestrates the run. A template is present under `configs/template_outer_config.yaml`.\n",
    "\n",
    "Here is how the template looks like:\n",
    "```{yaml}\n",
    "\n",
    "defaults:\n",
    "  - data_io: ??? # For new data, create data_io config and put its name here\n",
    "  - dataloader: ??? # For new data, create dataloader config and put its name here\n",
    "  - model: base_core_readout\n",
    "  - training_callbacks:\n",
    "    - early_stopping\n",
    "    - lr_monitor\n",
    "    - model_checkpoint\n",
    "  - logger:\n",
    "    - tensorboard\n",
    "    - csv\n",
    "  - trainer: default_deterministic\n",
    "  - hydra: default\n",
    "  - _self_ # values in this config will overwrite the defaults\n",
    "\n",
    "exp_name: example_experiment_new_data\n",
    "seed: 42\n",
    "check_stimuli_responses_match: false\n",
    "\n",
    "paths:\n",
    "  cache_dir: ${oc.env:OPENRETINA_CACHE_DIRECTORY} # Remote files are downloaded to this location\n",
    "  # If data_dir is a local path, data will be read from there. If a remote link, the target will be downloaded to cache_dir.\n",
    "  data_dir: ??? # Choose the location of the data. Should be used in data_io functions.\n",
    "  log_dir: \".\" # Used as parent for output_dir. Will store train logs.\n",
    "  output_dir: ${hydra:runtime.output_dir} # Modify in the \"hydra/default.yaml\" config\n",
    "\n",
    "# Overwrite model defaults with specifics for the current data input format\n",
    "model:\n",
    "  in_shape: ???\n",
    "  hidden_channels: ???\n",
    "  spatial_kernel_sizes: ??? \n",
    "  # Can over-ride further model defaults here.\n",
    "```\n",
    "\n",
    "#### Breaking down the template\n",
    "\n",
    "##### 1. Defaults section:\n",
    "- Hydra uses the defaults section to compose configurations from different files.\n",
    "- Each line here references a specific configuration file, stored in subdirectories within configs/.\n",
    "- For example, data_io: ??? means that a specific data_io config must be created and provided (e.g., maheswaranathan_2023).\n",
    "- Similarly, dataloader: ??? ensures that a dataloader configuration is selected.\n",
    "- _self_ ensures that values defined later in this file override the defaults.\n",
    "\n",
    "##### 2. Run specific variables\n",
    "- exp_name: The experiment name, which helps organize logs and outputs.\n",
    "- seed: A fixed seed for reproducibility.\n",
    "- check_stimuli_responses_match: A debugging flag to ensure that stimuli and responses are aligned correctly.\n",
    "\n",
    "##### 3. File paths\n",
    "- cache_dir: The base directory for downloads, if any need to happen.\n",
    "- data_dir: The location of the dataset, which can be referenced in data_io functions using `${paths.data_dir}`. If `${paths.data_dir}` is a remote path, its contents will be downloaded to cache_dir, and the downloaded files path will be used in loading the data.\n",
    "- log_dir: Parent folder for the logs, which is used by output_dir.\n",
    "- output_dir: Where logs, model checkpoints, and results will be saved. Uses logs dir as the parent, and sub-folder structure is set by hydra.\n",
    "\n",
    "##### 4. Model specific overrides\n",
    "This section defines the input shape and architecture details, overriding the default model configuration if needed.\n",
    "- ``in_shape``, ``hidden_channels``, and ``spatial_kernel_sizes`` are left as placeholders (???), meaning they should be specified based on the dataset used.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### Filling in the Configuration for `maheswaranathan_2023`\n",
    "Now, let’s see how this template is filled in for an actual experiment using `maheswaranathan_2023`:\n",
    "\n",
    "```{yaml}\n",
    "defaults:\n",
    "  - data_io: maheswaranathan_2023\n",
    "  - dataloader: maheswaranathan_2023\n",
    "  - model: base_core_readout\n",
    "  - training_callbacks:\n",
    "    - early_stopping\n",
    "    - lr_monitor\n",
    "    - model_checkpoint\n",
    "  - logger:\n",
    "    - tensorboard\n",
    "    - csv\n",
    "  - trainer: default_deterministic\n",
    "  - hydra: default\n",
    "  - _self_\n",
    "```\n",
    "\n",
    "Instead of ``???,`` we now explicitly specify ``maheswaranathan_2023`` for both ``data_io`` and ``dataloader``.\n",
    "The remaining configuration choices (e.g., logging, training callbacks, trainer) stay the same as the template, but could also be modified further. We provide different options in the respective folders.\n",
    "\n",
    "Continuing:\n",
    "```{yaml}\n",
    "exp_name: core_readout_maheswaranathan\n",
    "seed: 42\n",
    "check_stimuli_responses_match: false\n",
    "\n",
    "paths:\n",
    "  cache_dir: null # Assume we already downloaded and unzipped manually the data\n",
    "  data_dir: ${oc.env:HOME}/baccus_data/neural_code_data/ganglion_cell_data/ # Say we downloaded it in home\n",
    "  log_dir: \".\" # Save logs in the current directory\n",
    "  output_dir: ${hydra:runtime.output_dir} # Keep hydra default for sub-folders, which we set in configs/hydra/default.yaml\n",
    "\n",
    "model:\n",
    "  in_shape: [1, 100, 50, 50]\n",
    "  hidden_channels: [16, 32]\n",
    "  spatial_kernel_sizes: [15, 11]\n",
    "```\n",
    "- The experiment is now named \"core_readout_maheswaranathan\", which will be used in logs and outputs.\n",
    "- The dataset location is explicitly set to \"baccus_data/neural_code_data/ganglion_cell_data/\", where ``cache_dir`` should still be defined by the user.\n",
    "- The model section is defined, containing a few over-rides of the defaults for `base_core_readout`:\n",
    "  - ``in_shape: [1, 100, 50, 50]`` represents the input dimensions for the dataset.\n",
    "  - ``hidden_channels: [16, 32]`` defines the number of channels in each convolutional layer.\n",
    "  - ``spatial_kernel_sizes: [15, 11]`` specifies the spatial kernel sizes.\n",
    "\n",
    "---\n",
    "\n",
    "Once an outer config is specified, running training with the specified options is done via the command line with:\n",
    "\n",
    "```{bash}\n",
    "openretina train --config-name \"maheswaranathan_2023_core_readout\"\n",
    "```\n",
    "\n",
    "Where you need to change `\"maheswaranathan_2023_core_readout\"` with the name of your outer `YAML` config."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we walked through the process of integrating new dataset into `openretina` and getting started with training on it. While setting up a new dataset can be challenging, taking a structured approach makes it much more manageable, despite the initial learning curve. If you run into issues, don’t hesitate to reach out and explore further the Hydra and OpenRetina documentations for more details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_retina",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
