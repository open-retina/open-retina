{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will train a Core/Readout model on data from Hoefling et al., 2024: [\"A chromatic feature detector in the retina signals visual context changes\"](https://elifesciences.org/articles/86860).\n",
    "\n",
    "We will closely follow the structure of our unified training script, `openretina.cli.train.py`, including using Hydra to import and examine model config files. \n",
    "\n",
    "Note that using `openretina.cli.train.py`, and the corresponding command `openretina train` is the recommended way to run model training, as for some configurations it can take some time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import hydra\n",
    "import lightning\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from einops import rearrange\n",
    "\n",
    "from openretina.data_io.base import compute_data_info\n",
    "from openretina.data_io.cyclers import LongCycler, ShortCycler\n",
    "from openretina.data_io.hoefling_2024.dataloaders import natmov_dataloaders_v2\n",
    "from openretina.data_io.hoefling_2024.responses import filter_responses, make_final_responses\n",
    "from openretina.data_io.hoefling_2024.stimuli import movies_from_pickle\n",
    "from openretina.eval.metrics import correlation_numpy, feve\n",
    "from openretina.models.core_readout import CoreReadout\n",
    "from openretina.utils.file_utils import get_local_file_path\n",
    "from openretina.utils.h5_handling import load_h5_into_dict\n",
    "from openretina.utils.misc import CustomPrettyPrinter\n",
    "from openretina.utils.plotting import (\n",
    "    numpy_to_mp4_video,\n",
    ")\n",
    "\n",
    "matplotlib.rcParams[\"pdf.fonttype\"] = 42\n",
    "matplotlib.rcParams[\"ps.fonttype\"] = 42\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")  # to display logs in jupyter\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "pp = CustomPrettyPrinter(indent=4, max_lines=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import also the global config file for this model using hydra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with hydra.initialize(config_path=os.path.join(\"..\", \"configs\"), version_base=\"1.3\"):\n",
    "    cfg = hydra.compose(config_name=\"hoefling_2024_core_readout_low_res.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in loading data is determining from where it will be fetched / stored.\n",
    "\n",
    "Let's see how this is handled in the configs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(cfg.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The config contains the path from where files will be downloaded, and also requires the `root_dir` to be set by the user: this is the directory where the data will be stored, and where also model logs and checkpoints will end up.\n",
    "\n",
    "When using the training script, if root_dir is not set by the user in the config files or somewhere in the script, this will cause an error.\n",
    "\n",
    "The `root_dir` is also what the package will use in place of the default openretina cache folder. Let's set both here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_chosen_root_folder = \"/Data\"  # Change this with your desired path.\n",
    "\n",
    "cfg.data.root_dir = your_chosen_root_folder\n",
    "\n",
    "os.environ[\"OPENRETINA_CACHE_DIRECTORY\"] = your_chosen_root_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stimuli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading of the stimuli is achieved, in the training script, via:\n",
    "```\n",
    "movies_dict = hydra.utils.call(cfg.data_io.stimuli)\n",
    "```\n",
    "\n",
    "Let's unpack it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(cfg.data_io.stimuli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_path = get_local_file_path(file_path=cfg.data.movies_path, cache_folder=cfg.data.data_dir)\n",
    "\n",
    "movies_dict = movies_from_pickle(movies_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(movies_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also visualize a few seconds of the training video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_to_mp4_video(movies_dict.train[:, :300, ...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training script, responses are loaded through:\n",
    "\n",
    "```\n",
    "neuron_data_dict = hydra.utils.call(cfg.data_io.responses)\n",
    "```\n",
    "\n",
    "Let's unpack it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(cfg.data_io.responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this may look complex, it effectively amounts to resolving a few intermediate steps in loading the data, and should be read from the inside out.\n",
    "\n",
    "When written more simply, it is equivalent to the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_path = get_local_file_path(file_path=cfg.data.responses_path, cache_folder=cfg.data.data_dir)\n",
    "\n",
    "responses_dict = load_h5_into_dict(file_path=responses_path)\n",
    "\n",
    "filtered_responses_dict = filter_responses(responses_dict, **cfg.quality_checks)\n",
    "\n",
    "final_responses = make_final_responses(filtered_responses_dict, response_type=\"natural\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how the final responses will be organised:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(final_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding code in `train.py` is:\n",
    "```\n",
    "dataloaders = hydra.utils.instantiate(\n",
    "        cfg.dataloader,\n",
    "        neuron_data_dictionary=neuron_data_dict,\n",
    "        movies_dictionary=movies_dict,\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(cfg.dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = natmov_dataloaders_v2(\n",
    "    neuron_data_dictionary=final_responses,\n",
    "    movies_dictionary=movies_dict,\n",
    "    allow_over_boundaries=True,\n",
    "    batch_size=128,\n",
    "    train_chunk_size=50,\n",
    "    validation_clip_indices=cfg.dataloader.validation_clip_indices,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(dataloaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also compute `data_info`, which is used to initialise certain model components and to save important metadata about stimuli and responses within the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info = compute_data_info(neuron_data_dictionary=final_responses, movies_dictionary=movies_dict)\n",
    "\n",
    "pp.pprint(data_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevant `train.py` section:\n",
    "```\n",
    "cfg.model.n_neurons_dict = data_info[\"n_neurons_dict\"]\n",
    "\n",
    "model = hydra.utils.instantiate(cfg.model, data_info=data_info)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The config for the model will contain all the relevant hyperparameters for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(cfg.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the value for `n_neurons_dict` is missing, and needs to be set from data_info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons_dict = data_info[\"n_neurons_dict\"]\n",
    "\n",
    "model = CoreReadout(\n",
    "    in_shape=(2, 150, 18, 16),\n",
    "    hidden_channels=(16, 16),\n",
    "    temporal_kernel_sizes=(21, 11),\n",
    "    spatial_kernel_sizes=(11, 5),\n",
    "    n_neurons_dict=n_neurons_dict,\n",
    "    core_gamma_hidden=0.0,\n",
    "    core_gamma_in_sparse=0.0,\n",
    "    core_gamma_input=0.0,\n",
    "    core_gamma_temporal=40.0,\n",
    "    core_hidden_padding=True,\n",
    "    core_input_padding=False,\n",
    "    cut_first_n_frames_in_core=0,\n",
    "    downsample_input_kernel_size=None,\n",
    "    dropout_rate=0.0,\n",
    "    learning_rate=0.01,\n",
    "    maxpool_every_n_layers=None,\n",
    "    readout_bias=True,\n",
    "    readout_gamma=0.4,\n",
    "    readout_gaussian_masks=True,\n",
    "    readout_gaussian_mean_scale=6.0,\n",
    "    readout_gaussian_var_scale=4.0,\n",
    "    readout_positive=True,\n",
    "    readout_scale=True,\n",
    "    data_info=data_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With data imported, models initialised and dataloaders set up, we can turn to training. \n",
    "\n",
    "```\n",
    "log_folder = os.path.join(cfg.data.output_dir, cfg.exp_name)\n",
    "os.makedirs(log_folder, exist_ok=True)\n",
    "logger_array = []\n",
    "for _, logger_params in cfg.logger.items():\n",
    "    logger = hydra.utils.instantiate(logger_params, save_dir=log_folder)\n",
    "    logger_array.append(logger)\n",
    "\n",
    "callbacks = [\n",
    "    hydra.utils.instantiate(callback_params) for callback_params in cfg.get(\"training_callbacks\", {}).values()\n",
    "]\n",
    "\n",
    "trainer = hydra.utils.instantiate(cfg.trainer, logger=logger_array, callbacks=callbacks)\n",
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=valid_loader)\n",
    "```\n",
    "\n",
    "This section is a bit more involved in `train.py`, to leave flexibility for different loggers and callbacks configurations. We are going to keep it simple here.\n",
    "\n",
    "Let's first initialise a simple tensorboard logger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_save_path = os.path.join(cfg.data.output_dir, \"notebook_example\")\n",
    "os.makedirs(log_save_path, exist_ok=True)\n",
    "\n",
    "logger = lightning.pytorch.loggers.TensorBoardLogger(\n",
    "    name=\"tensorboard/\",\n",
    "    save_dir=log_save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then some training callbacks (i.e. utility functions that will be called during training):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = lightning.pytorch.callbacks.EarlyStopping(\n",
    "    monitor=\"val_correlation\",\n",
    "    patience=10,\n",
    "    mode=\"max\",\n",
    "    verbose=False,\n",
    "    min_delta=0.001,\n",
    ")\n",
    "\n",
    "lr_monitor = lightning.pytorch.callbacks.LearningRateMonitor(logging_interval=\"epoch\")\n",
    "\n",
    "model_checkpoint = lightning.pytorch.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_correlation\", mode=\"max\", save_weights_only=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then instantiate the trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = lightning.Trainer(max_epochs=100, logger=logger, callbacks=[early_stopping, lr_monitor, model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can start training. Before doing sp though, we can initialise the tensorboard jupyter integration, to visualize how training progresses.\n",
    "\n",
    "Run the following cell once or twice until the tensorboard extension UI shows up. Once is shows, note that at the beginning it will show no data (unless you have run this notebook before), because we have not started the trainer yet.\n",
    "\n",
    "When you run the cell containing `trainer.fit` you can then come back to the tensorboard extension, reload the window *within the extension* by clicking the refresh icon in the top right, and follow the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir {log_save_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only last important step before calling the trainer is to convert the dictionary of dataloaders we have into a unified iterator that will cycle through all sessions during training and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = LongCycler(dataloaders[\"train\"])\n",
    "val_loader = ShortCycler(dataloaders[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are finally ready to train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is done training, we can turn to evaluation.\n",
    "\n",
    "First, let's still use the trainer to see the poisson and correlation performance on each of the dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = ShortCycler(dataloaders[\"test\"])\n",
    "trainer.test(model, dataloaders=[train_loader, val_loader, test_loader], ckpt_path=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at further evals, like the fraction of explainable variance explained for an example session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pick an example session\n",
    "example_session = list(final_responses.keys())[0]\n",
    "\n",
    "# Extract responses by trial:\n",
    "responses_by_trial = final_responses[example_session].test_by_trial\n",
    "\n",
    "responses_by_trial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the test movie for that session:\n",
    "test_movie = dataloaders[\"test\"][example_session].dataset.movies\n",
    "\n",
    "# Pass it through the model: move to gpu and add batch dimension\n",
    "with torch.no_grad():\n",
    "    model_predictions = model.forward(test_movie.to(model.device).unsqueeze(0), data_key=example_session)\n",
    "\n",
    "model_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(feve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to reshape the predictions and responses by trial to match what the function expects\n",
    "\n",
    "feve_score = feve(\n",
    "    rearrange(responses_by_trial, \"neurons time trials -> time trials neurons\")[20:],\n",
    "    model_predictions.squeeze(0).cpu().numpy(),\n",
    ")\n",
    "\n",
    "print(f\"Average FEVe score for session {example_session}: {feve_score.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot an example neuron's predictions and its ground truth response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_idx = 4\n",
    "session_idx = 0\n",
    "\n",
    "\n",
    "example_session = list(final_responses.keys())[session_idx]\n",
    "\n",
    "test_sample = next(iter(dataloaders[\"test\"][example_session]))\n",
    "responses_by_trial = final_responses[example_session].test_by_trial\n",
    "mean_test_responses = final_responses[example_session].test\n",
    "\n",
    "input_samples = test_sample.inputs\n",
    "targets = test_sample.targets\n",
    "\n",
    "model.eval()\n",
    "model.cpu()\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructions = model(input_samples.cpu(), example_session)\n",
    "reconstructions = reconstructions.cpu().numpy().squeeze()\n",
    "\n",
    "feve_score = feve(\n",
    "    rearrange(responses_by_trial, \"neurons time trials -> time trials neurons\")[20:],\n",
    "    model_predictions.squeeze(0).cpu().numpy(),\n",
    ")\n",
    "\n",
    "correlations = correlation_numpy(mean_test_responses.T[20:], model_predictions.squeeze(0).cpu().numpy(), axis=0)\n",
    "\n",
    "\n",
    "targets = targets.cpu().numpy().squeeze()\n",
    "window = 750\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.arange(0, window), targets[:window, neuron_idx], label=\"target\")\n",
    "plt.plot(np.arange(20, window), reconstructions[:window, neuron_idx], label=\"prediction\")\n",
    "plt.suptitle(f\"Neuron {neuron_idx} - FEVE: {feve_score[neuron_idx]:.2f} - Correlation: {correlations[neuron_idx]:.2f}\")\n",
    "\n",
    "plt.legend()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
