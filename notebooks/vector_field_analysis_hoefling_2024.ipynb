{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40797bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import hydra\n",
    "\n",
    "from openretina.data_io.hoefling_2024.stimuli import movies_from_pickle\n",
    "from openretina.insilico.VectorFieldAnalysis.vector_field_analysis import *\n",
    "from openretina.models.core_readout import load_core_readout_from_remote\n",
    "from openretina.utils.file_utils import get_local_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77de5502",
   "metadata": {},
   "source": [
    "In this notebook, we will introduce how to use OpenRetina to compute the vector field of local Spike Triggered Average as described in Goldin et al. 2022. This analysis can be used as a complement to the Most Discriminative Stimulus analysis The method should work for most models trained in the OpenRetina framework but the user might need to readapt how the dataset of natural images is constructed (usually from snippets of the training dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a3b828",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d105f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_quality = \"low\"\n",
    "model_name = f\"hoefling_2024_base_{model_quality}_res\"\n",
    "model = load_core_readout_from_remote(\n",
    "    model_name, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1c1dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a session if needed\n",
    "session_id = 'session_3_ventral2_20210910'  # list(model.readout.keys())[0]\n",
    "print('Using session : ', session_id)\n",
    "n_neurons = model.readout[session_id].outdims\n",
    "print(f\"Number of neurons: {n_neurons}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c03e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = list(model.readout.keys())\n",
    "\n",
    "# Optional / We will reshape the cell type annotations into a new format to use it later down\n",
    "group_sessions_cells = {}\n",
    "\n",
    "# Iterate through all sessions\n",
    "for session in sessions:\n",
    "    # Get assignments for this session\n",
    "    assignments = model.data_info[\"sessions_kwargs\"][session][\"group_assignment\"]\n",
    "\n",
    "    # Get number of neurons for this session\n",
    "    n_neurons = model.readout[session].outdims\n",
    "\n",
    "    # For each cell in this session\n",
    "    for cell_id in range(n_neurons):\n",
    "        group = int(assignments[cell_id])  # Convert to int for dictionary key\n",
    "\n",
    "        # Initialize group if not exists\n",
    "        if group not in group_sessions_cells:\n",
    "            group_sessions_cells[group] = {}\n",
    "\n",
    "        # Initialize session if not exists for this group\n",
    "        if session not in group_sessions_cells[group]:\n",
    "            group_sessions_cells[group][session] = []\n",
    "\n",
    "        # Add cell ID to the appropriate group and session\n",
    "        group_sessions_cells[group][session].append(cell_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60211cda",
   "metadata": {},
   "source": [
    "# Load natural images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237b397d",
   "metadata": {},
   "source": [
    "We need to create a dataset of natural images to which we will compute the lSTA to. For movie models we can simply take random frames from the movies. We recommend using ~3000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58277a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the case of Hoefling et al, let's load training data\n",
    "with hydra.initialize(config_path=os.path.join(\"..\", \"configs\"), version_base=\"1.3\"):\n",
    "    cfg = hydra.compose(config_name=f\"hoefling_2024_core_readout_{model_quality}_res.yaml\")\n",
    "\n",
    "movies_path = get_local_file_path(file_path=cfg.paths.movies_path, cache_folder=cfg.paths.data_dir)\n",
    "\n",
    "movies_dict = movies_from_pickle(movies_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd1ef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will just take equally intervleaved frames from the training movies to create a dataset of natural images.\n",
    "n_images = 3000\n",
    "rate = movies_dict.train.shape[1] // n_images\n",
    "natural_images_library = movies_dict.train[:, ::rate, :, :].swapaxes(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a0968",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# The next function will turn our images into singular sequence grey -> frame, using a length of grey depending on the size\n",
    "# of the model temporal filtering\n",
    "# The n_image_frames is the number of frames the natural image is displayed, depending on the window you want to average\n",
    "# the cell response for LSTA computation (see below).\n",
    "movies, n_empty_frames = prepare_movies_dataset(model, session_id,\n",
    "    normalize_movies=False, # Already normalized\n",
    "    image_library=natural_images_library, n_image_frames=60, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8903e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the last frame of an example movie with both channels side by side\n",
    "example_idx = 150  # First movie as example\n",
    "last_frame = movies[example_idx, :, -1] # movies[example_idx, :, -1]  # Shape: (2, 72, 64)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# First channel (channel 0)\n",
    "im1 = axes[0].imshow(last_frame[0], cmap='Greens', vmin=movies[:, 0].min(), vmax=movies[:, 0].max())\n",
    "axes[0].set_title('First Channel')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "axes[0].axis('off')\n",
    "# Optional : Second channel (channel 1)\n",
    "im2 = axes[1].imshow(last_frame[1], cmap='Purples', vmin=movies[:, 1].min(), vmax=movies[:, 1].max())\n",
    "axes[1].set_title('Second Channel')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "axes[1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"First channel range: [{last_frame[0].min():.3f}, {last_frame[0].max():.3f}]\")\n",
    "print(f\"Second channel range: [{last_frame[1].min():.3f}, {last_frame[1].max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1aaf55",
   "metadata": {},
   "source": [
    "# Draw the vector field for a selected cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3ea80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_id = 11  # TO DO: CHOOSE BETTER EXQMPLE\n",
    "\n",
    "# The following function will compute the response and the lSTA to all the images in the movies dataset\n",
    "# Choose an integration window that makes sense for your model (here 5-40 frames after image onset, i.e. 0.17s to 1.33s\n",
    "#  after image onset)\n",
    "lsta_library, response_library = compute_lsta_library(\n",
    "    model,\n",
    "    movies[:n_images], # Can limit number of images for it to run faster\n",
    "    session_id,\n",
    "    cell_id,\n",
    "    integration_window=(0, 30),  # Integration window for the LSTA\n",
    "    batch_size=64,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b97d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsta_library.shape, response_library.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c05a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can check some sample responses, making sure the integrity of the response profile is predicted and we did not\n",
    "#  cut it too short.\n",
    "# This view can be used to help select the integration window (typical number of frames on the cell response\n",
    "#  in the dataset)\n",
    "\n",
    "plt.plot(response_library[0, :, cell_id])\n",
    "plt.plot(response_library[1, :, cell_id])\n",
    "plt.plot(response_library[2, :, cell_id])\n",
    "plt.plot(response_library[3, :, cell_id])\n",
    "plt.plot(response_library[4, :, cell_id])\n",
    "plt.plot(response_library[5, :, cell_id])\n",
    "plt.xlabel('Time (frames)')\n",
    "plt.ylabel('Cell predicted Response (au)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a694292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now plot an example LSTA and the corresponding image\n",
    "\n",
    "image = 450\n",
    "channel = 1\n",
    "lsta = lsta_library[image, channel]\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# image\n",
    "axes[0].imshow(movies[image,0,-1], cmap='gray')\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# LSTA\n",
    "axes[1].imshow(lsta, cmap='bwr', vmin=-abs(lsta).max(), vmax=abs(lsta).max())\n",
    "axes[1].set_title('LSTA (Local Spatiotemporal Average)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47984ab4",
   "metadata": {},
   "source": [
    "# PCA on the LSTA library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51773d82",
   "metadata": {},
   "source": [
    "Interpretation : Here we are drawing a approximation of what are the spatial components of the cell lSTA. They are the two axis directing the cell selectivity, usually the first PC looks like a simple receptive field and can be seen as a 'luminance' axis, while the second PC is more complex spatially and can be seen as a 'local contrast' axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e6ae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select channel if using a color channels model\n",
    "channel = 0\n",
    "\n",
    "PC1, PC2, explained_variance = get_pc_from_pca(model, channel, lsta_library, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf190a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the images onto PCA space\n",
    "images = movies[:,channel,-1,:,:]\n",
    "images_coordinate = get_images_coordinate(images, PC1, PC2, plot=True)\n",
    "plt.axis('off')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69e3f69",
   "metadata": {},
   "source": [
    "# Plot the vector field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402e7d88",
   "metadata": {},
   "source": [
    "Interpretation :\n",
    "We can project both the images (represented as arrow basis) and the corresponding lSTAs (represented as arrow heads) on the two first PCs. This give use a view of the geometry of the response space of the cell. If the arrow all points in the same horizontal direction, the cell consistently encode the same thing. If the directions arrows vary, the cell selectivity depends on the current context. Arrows diverging can be interpreted as a preference for increase in local contrast in a part of stimulus space, while arrows convergind could be a preference for decrease in local contrast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304dfce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL : Plot the raw vector field of the LSTA in PCA space\n",
    "# fig = plot_untreated_vectorfield(lsta_library, channel, PC1, PC2, images_coordinate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d39a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the optimiz vector field of the LSTA in PCA space\n",
    "fig = plot_clean_vectorfield(\n",
    "    lsta_library,\n",
    "    channel,\n",
    "    PC1,\n",
    "    PC2,\n",
    "    images,\n",
    "    images_coordinate,\n",
    "    explained_variance,\n",
    "    x_bins=30,\n",
    "    y_bins=30,\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# ADD the firing rate by the side + mention cell types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31f3a6c",
   "metadata": {},
   "source": [
    "# Optional : Create all arrowplots of a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4a1c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for session_id in list(model.readout.keys())[13:]:\n",
    "#     os.makedirs(f\"hoefling_low_res_vector_fields/{session_id}\", exist_ok=True)\n",
    "#     n_neurons = model.readout[session_id].outdims\n",
    "#     print(n_neurons)\n",
    "#     movies, n_empty_frames = prepare_movies_dataset(\n",
    "#         model,\n",
    "#         session_id,\n",
    "#         normalize_movies=False,  # Already normalized\n",
    "#         image_library=natural_images_library,\n",
    "#         n_image_frames=60,\n",
    "#         device=device,\n",
    "#     )\n",
    "\n",
    "#     for cell_id in range(n_neurons):\n",
    "#         print(f\"Processing cell {cell_id}...\")\n",
    "#         lsta_library, response_library = compute_lsta_library(\n",
    "#             model, movies, session_id, cell_id, batch_size=64, device=device\n",
    "#         )\n",
    "#         assignment = model.data_info['sessions_kwargs'][session_id]['group_assignment'][cell_id]\n",
    "#         for channel in range(2):\n",
    "#             PC1, PC2, explained_variance = get_pc_from_pca(model, channel, lsta_library, plot=True)\n",
    "#             images_coordinate = get_images_coordinate(images, PC1, PC2, plot=False)\n",
    "#             fig = plot_clean_vectorfield(\n",
    "#                 lsta_library,\n",
    "#                 channel,\n",
    "#                 PC1,\n",
    "#                 PC2,\n",
    "#                 images,\n",
    "#                 images_coordinate,\n",
    "#                 explained_variance,\n",
    "#                 x_bins=31,\n",
    "#                 y_bins=31,\n",
    "#             )\n",
    "#             plt.savefig(\n",
    "#                 f\"hoefling_low_res_vector_fields/{session_id}/cell_{cell_id}_channel_{channel}_group_{assignment}.png\",\n",
    "#                 dpi=300,\n",
    "#                 bbox_inches=\"tight\",\n",
    "#             )\n",
    "#             plt.close(\"all\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_retina",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
