{
 "cells": [
  {
   "cell_type": "code",
   "id": "a93916c08fe75e08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T18:50:24.643657478Z",
     "start_time": "2025-12-12T18:50:06.767674015Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENRETINA_CACHE_DIRECTORY']  = '/projects/extern/nhr/nhr_ni/nim00010/dir.project/'\n",
    "import hydra\n",
    "import lightning\n",
    "import wandb\n",
    "\n",
    "from openretina.data_io.cyclers import LongCycler\n",
    "from openretina.models.core_readout import UnifiedCoreReadout\n",
    "from openretina.utils.model_utils import get_core_output_based_on_dimensions\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/vystrcilova/u14647/.conda/envs/venv_or/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "913f45452bdb4cb4",
   "metadata": {},
   "source": [
    "##  Initialize config"
   ]
  },
  {
   "cell_type": "code",
   "id": "68137d5532b3e567",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T18:50:39.305038480Z",
     "start_time": "2025-12-12T18:50:38.965909245Z"
    }
   },
   "source": [
    "config_name = \"vystrcilova_2024_wn_cnn.yaml\"  # use vystrcilova_2024_nm_cnn.yaml for the natural movie dataset\n",
    "\n",
    "with hydra.initialize(config_path=\"../configs\", version_base=\"1.3\"):\n",
    "    cfg = hydra.compose(config_name=config_name)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "6a50c10beee4092f",
   "metadata": {},
   "source": [
    "## Downloading datasets\n",
    "Dataset download will happen only once and automatically, the first time you attempt to instantiate the dataloader, at the location specified by `cfg.cache_dir`.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "01de35e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:32:09.878773Z",
     "start_time": "2025-12-01T23:32:09.873432Z"
    }
   },
   "source": [
    "cfg.paths.cache_dir"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/projects/extern/nhr/nhr_ni/nim00010/dir.project/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T18:51:49.082714992Z",
     "start_time": "2025-12-12T18:51:36.496033736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "movies_dict = hydra.utils.call(cfg.data_io.stimuli)\n",
    "neuron_data_dict = hydra.utils.call(cfg.data_io.responses)\n",
    "\n",
    "if cfg.check_stimuli_responses_match:\n",
    "    for session, neuron_data in neuron_data_dict.items():\n",
    "        neuron_data.check_matching_stimulus(movies_dict[session])"
   ],
   "id": "524963de852f6bc2",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "4b4bf34d4f4add93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T18:51:59.987586434Z",
     "start_time": "2025-12-12T18:51:51.750959268Z"
    }
   },
   "source": [
    "print(cfg)\n",
    "cfg.dataloader.num_of_frames = cfg.model.core.temporal_kernel_sizes[0]\n",
    "cfg.dataloader.num_of_layers = len(cfg.model.core.temporal_kernel_sizes)\n",
    "dataloaders = hydra.utils.instantiate(cfg.dataloader)\n",
    "n_neurons_dict = {}\n",
    "retina_indices = list(dataloaders[\"train\"].keys())\n",
    "for index in retina_indices:\n",
    "    n_neurons_dict[index] = dataloaders[\"train\"][index].dataset.n_neurons\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_io': {'sessions': ['01', '02', '04'], 'input_channels': 1, 'placeholder_time_bins': 100, 'placeholder_height': 80, 'placeholder_width': 90, 'stimulus_seed': 0, 'response_files': {'01': 'responses/cell_responses_01_wn.pkl', '02': 'responses/cell_responses_02_wn.pkl', '04': 'responses/cell_responses_04_wn.pkl'}, 'excluded_cells': {'01': [], '02': [], '04': []}, 'stimuli': {'_target_': 'openretina.data_io.sridhar_2025.stimuli.build_placeholder_movies', 'session_ids': '${data_io.sessions}', 'channels': '${data_io.input_channels}', 'height': '${data_io.placeholder_height}', 'width': '${data_io.placeholder_width}', 'time_bins': '${data_io.placeholder_time_bins}', 'stim_id_prefix': 'sridhar_wn_2025'}, 'responses': {'_target_': 'openretina.data_io.sridhar_2025.responses.response_splits_from_pickles', 'base_path': '${paths.data_dir}', 'files': '${data_io.response_files}', 'excluded_cells': '${data_io.excluded_cells}', 'stimulus_seed': '${data_io.stimulus_seed}'}, 'data_info': {'response_rate_hz': '85 Hz', 'stimulus_rate_hz': '85 Hz', 'stimulus_class': 'whitenoise', 'retina_pixel_size_um': 30}}, 'dataloader': {'_target_': 'openretina.data_io.sridhar_2025.dataloaders.white_noise_loader', '_convert_': 'object', 'basepath': '${paths.cache_dir}', 'files': {'01': 'responses/cell_responses_01_wn.pkl', '02': 'responses/cell_responses_02_wn.pkl', '04': 'responses/cell_responses_04_wn.pkl'}, 'big_crops': {'01': [35, 35, 55, 55], '02': [35, 35, 55, 55], '04': [35, 35, 55, 55]}, 'train_image_path': {'01': 'non_repeating_stimuli_1', '02': 'non_repeating_stimuli_1', '04': 'non_repeating_stimuli_2'}, 'test_image_path': {'01': 'repeating_stimuli_1', '02': 'repeating_stimuli_1', '04': 'repeating_stimuli_2'}, 'excluded_cells': {'01': [], '02': [], '03': [], '04': []}, 'batch_size': 16, 'seed': None, 'train_frac': 0.8, 'subsample': 1, 'crop': 0, 'use_cache': True, 'cache_maxsize': 1, 'num_of_trials_to_use': None, 'start_using_trial': 0, 'num_of_frames': '${model.core.temporal_kernel_sizes}', 'temporal_dilation': 1, 'hidden_temporal_dilation': 1, 'cell_index': None, 'device': 'cuda', 'time_chunk_size': 100, 'num_of_layers': 1, 'retina_specific_crops': True, 'extra_frame': 0, 'hard_coded': None, 'retina_index': None, 'chunked_sampling': True}, 'model': {'hidden_channels': [16], 'in_shape': ['${data_io.input_channels}', '${data_io.placeholder_time_bins}', '${data_io.placeholder_height}', '${data_io.placeholder_width}'], 'n_neurons_dict': '???', 'core': {'_target_': 'openretina.modules.core.base_core.SimpleCoreWrapper', '_convert_': 'object', 'temporal_kernel_sizes': [21], 'spatial_kernel_sizes': [11], 'gamma_input': 0.0, 'gamma_in_sparse': 0.0, 'gamma_hidden': 0.0, 'gamma_temporal': 40.0, 'input_padding': False, 'hidden_padding': [0, 0, 0], 'maxpool_every_n_layers': None, 'downsample_input_kernel_size': None, 'convolution_type': 'custom_separable', 'dropout_rate': 0.0, 'cut_first_n_frames': 0, 'channels': '???'}, 'readout': {'_target_': 'openretina.modules.readout.multi_readout.MultiSampledGaussianReadout', '_convert_': 'object', 'in_shape': '???', 'n_neurons_dict': '???', 'bias': True, 'gamma': 0.4, 'init_mu_range': 0.1, 'init_sigma_range': 0.15, 'reg_avg': False, 'batch_sample': True, 'align_corners': True, 'gauss_type': 'full', 'grid_mean_predictor': None, 'shared_features': None, 'init_grid': None, 'shared_grid': None}, 'learning_rate': 0.01}, 'training_callbacks': {'early_stopping': {'_target_': 'lightning.pytorch.callbacks.EarlyStopping', 'monitor': 'val_correlation', 'min_delta': 0.001, 'patience': 10, 'verbose': True, 'mode': 'max', 'strict': True, 'check_finite': False}, 'lr_monitor': {'_target_': 'lightning.pytorch.callbacks.LearningRateMonitor', 'logging_interval': 'epoch', 'log_momentum': True, 'log_weight_decay': True}, 'model_checkpoint': {'_target_': 'lightning.pytorch.callbacks.ModelCheckpoint', 'dirpath': '${paths.output_dir}/checkpoints', 'filename': '{epoch:02d}_{val_correlation:.3f}', 'monitor': 'val_correlation', 'verbose': False, 'save_last': False, 'save_top_k': 1, 'mode': 'max', 'auto_insert_metric_name': True, 'save_weights_only': False, 'every_n_train_steps': None, 'train_time_interval': None, 'every_n_epochs': None, 'save_on_train_epoch_end': None}}, 'logger': {'csv': {'_target_': 'lightning.pytorch.loggers.csv_logs.CSVLogger', 'save_dir': '${paths.output_dir}', 'name': 'csv/', 'prefix': '', 'version': ''}}, 'trainer': {'_target_': 'lightning.Trainer', 'default_root_dir': '${paths.output_dir}', 'deterministic': 'warn', 'max_epochs': 100, 'gradient_clip_val': 1, 'precision': '32-true', 'accumulate_grad_batches': 1}, 'exp_name': 'vystrcilova_2024_wn_core_readout', 'seed': 42, 'check_stimuli_responses_match': False, 'paths': {'load_model_path': None, 'cache_dir': '/scratch-grete/projects/nim00010/data/gollisch_lab/sridhar_2025/marmoset/whitenoise', 'data_dir': 'https://huggingface.co/datasets/open-retina/open-retina/tree/main/gollisch_lab/sridhar_2025/marmoset/whitenoise', 'log_dir': './logs/cnn_wn', 'output_dir': '${hydra:runtime.output_dir}'}, 'matmul_precision': {'_target_': 'torch.set_float32_matmul_precision', 'precision': 'highest'}, 'only_train_readout': False}\n",
      "Random seed 1000 has been set.\n",
      "train idx: [7 6 9 1 2 8 0 3]\n",
      "val idx: [ 4 10  5]\n"
     ]
    },
    {
     "ename": "InstantiationException",
     "evalue": "Error in call to target 'openretina.data_io.sridhar_2025.dataloaders.white_noise_loader':\nTypeError(\"'int' object is not subscriptable\")\nfull_key: dataloader",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.conda/envs/venv_or/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py:92\u001B[39m, in \u001B[36m_call_target\u001B[39m\u001B[34m(_target_, _partial_, args, kwargs, full_key)\u001B[39m\n\u001B[32m     91\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m92\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_target_\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     93\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/scratch-grete/projects/nim00010/open-retina/openretina/data_io/sridhar_2025/dataloaders.py:1134\u001B[39m, in \u001B[36mwhite_noise_loader\u001B[39m\u001B[34m(basepath, files, big_crops, train_image_path, test_image_path, excluded_cells, batch_size, seed, train_frac, subsample, crop, use_cache, cache_maxsize, retina_index, num_of_trials_to_use, start_using_trial, num_of_frames, temporal_dilation, hidden_temporal_dilation, cell_index, device, time_chunk_size, num_of_layers, retina_specific_crops, extra_frame, hard_coded, chunked_sampling, get_locations, sta_dir, **kwargs)\u001B[39m\n\u001B[32m   1112\u001B[39m     locations = get_locations_from_stas(\n\u001B[32m   1113\u001B[39m         sta_dir=os.path.join(basepath, sta_dir),\n\u001B[32m   1114\u001B[39m         retina_index=retina_index,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1123\u001B[39m         flip_sta=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m   1124\u001B[39m     )\n\u001B[32m   1125\u001B[39m train_loader = get_noise_dataloader(\n\u001B[32m   1126\u001B[39m     {\u001B[33m\"\u001B[39m\u001B[33mtrain_responses\u001B[39m\u001B[33m\"\u001B[39m: train_responses, \u001B[33m\"\u001B[39m\u001B[33mtest_responses\u001B[39m\u001B[33m\"\u001B[39m: test_responses},\n\u001B[32m   1127\u001B[39m     path=dataset_train_image_path,\n\u001B[32m   1128\u001B[39m     indices=train_ids,\n\u001B[32m   1129\u001B[39m     test=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m   1130\u001B[39m     shuffle=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m   1131\u001B[39m     batch_size=batch_size,\n\u001B[32m   1132\u001B[39m     use_cache=use_cache,\n\u001B[32m   1133\u001B[39m     cache_maxsize=cache_maxsize,\n\u001B[32m-> \u001B[39m\u001B[32m1134\u001B[39m     num_of_frames=\u001B[43mnum_of_frames\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m,\n\u001B[32m   1135\u001B[39m     num_of_hidden_frames=num_of_frames[\u001B[32m1\u001B[39m:] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(num_of_frames) > \u001B[32m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1136\u001B[39m     device=device,\n\u001B[32m   1137\u001B[39m     crop=crop,\n\u001B[32m   1138\u001B[39m     subsample=subsample,\n\u001B[32m   1139\u001B[39m     time_chunk_size=time_chunk_size,\n\u001B[32m   1140\u001B[39m     num_of_layers=num_of_layers,\n\u001B[32m   1141\u001B[39m     temporal_dilation=temporal_dilation,\n\u001B[32m   1142\u001B[39m     hidden_temporal_dilation=hidden_temporal_dilation,\n\u001B[32m   1143\u001B[39m     extra_frame=extra_frame,\n\u001B[32m   1144\u001B[39m     chunked_sampling=chunked_sampling,\n\u001B[32m   1145\u001B[39m     locations=locations,\n\u001B[32m   1146\u001B[39m     excluded_cells=excluded_cells,\n\u001B[32m   1147\u001B[39m )\n\u001B[32m   1149\u001B[39m valid_loader = get_noise_dataloader(\n\u001B[32m   1150\u001B[39m     {\u001B[33m\"\u001B[39m\u001B[33mtrain_responses\u001B[39m\u001B[33m\"\u001B[39m: train_responses, \u001B[33m\"\u001B[39m\u001B[33mtest_responses\u001B[39m\u001B[33m\"\u001B[39m: test_responses},\n\u001B[32m   1151\u001B[39m     path=dataset_train_image_path,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1170\u001B[39m     excluded_cells=excluded_cells,\n\u001B[32m   1171\u001B[39m )\n",
      "\u001B[31mTypeError\u001B[39m: 'int' object is not subscriptable",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mInstantiationException\u001B[39m                    Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      2\u001B[39m cfg.dataloader.num_of_frames = cfg.model.core.temporal_kernel_sizes[\u001B[32m0\u001B[39m]\n\u001B[32m      3\u001B[39m cfg.dataloader.num_of_layers = \u001B[38;5;28mlen\u001B[39m(cfg.model.core.temporal_kernel_sizes)\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m dataloaders = \u001B[43mhydra\u001B[49m\u001B[43m.\u001B[49m\u001B[43mutils\u001B[49m\u001B[43m.\u001B[49m\u001B[43minstantiate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m n_neurons_dict = {}\n\u001B[32m      6\u001B[39m retina_indices = \u001B[38;5;28mlist\u001B[39m(dataloaders[\u001B[33m\"\u001B[39m\u001B[33mtrain\u001B[39m\u001B[33m\"\u001B[39m].keys())\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.conda/envs/venv_or/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py:226\u001B[39m, in \u001B[36minstantiate\u001B[39m\u001B[34m(config, *args, **kwargs)\u001B[39m\n\u001B[32m    223\u001B[39m     _convert_ = config.pop(_Keys.CONVERT, ConvertMode.NONE)\n\u001B[32m    224\u001B[39m     _partial_ = config.pop(_Keys.PARTIAL, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m--> \u001B[39m\u001B[32m226\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minstantiate_node\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    227\u001B[39m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrecursive\u001B[49m\u001B[43m=\u001B[49m\u001B[43m_recursive_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m=\u001B[49m\u001B[43m_convert_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpartial\u001B[49m\u001B[43m=\u001B[49m\u001B[43m_partial_\u001B[49m\n\u001B[32m    228\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    229\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m OmegaConf.is_list(config):\n\u001B[32m    230\u001B[39m     \u001B[38;5;66;03m# Finalize config (convert targets to strings, merge with kwargs)\u001B[39;00m\n\u001B[32m    231\u001B[39m     config_copy = copy.deepcopy(config)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.conda/envs/venv_or/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py:347\u001B[39m, in \u001B[36minstantiate_node\u001B[39m\u001B[34m(node, convert, recursive, partial, *args)\u001B[39m\n\u001B[32m    342\u001B[39m                 value = instantiate_node(\n\u001B[32m    343\u001B[39m                     value, convert=convert, recursive=recursive\n\u001B[32m    344\u001B[39m                 )\n\u001B[32m    345\u001B[39m             kwargs[key] = _convert_node(value, convert)\n\u001B[32m--> \u001B[39m\u001B[32m347\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_call_target\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_target_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpartial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfull_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    348\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    349\u001B[39m     \u001B[38;5;66;03m# If ALL or PARTIAL non structured or OBJECT non structured,\u001B[39;00m\n\u001B[32m    350\u001B[39m     \u001B[38;5;66;03m# instantiate in dict and resolve interpolations eagerly.\u001B[39;00m\n\u001B[32m    351\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m convert == ConvertMode.ALL \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[32m    352\u001B[39m         convert \u001B[38;5;129;01min\u001B[39;00m (ConvertMode.PARTIAL, ConvertMode.OBJECT)\n\u001B[32m    353\u001B[39m         \u001B[38;5;129;01mand\u001B[39;00m node._metadata.object_type \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28mdict\u001B[39m)\n\u001B[32m    354\u001B[39m     ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.conda/envs/venv_or/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py:97\u001B[39m, in \u001B[36m_call_target\u001B[39m\u001B[34m(_target_, _partial_, args, kwargs, full_key)\u001B[39m\n\u001B[32m     95\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m full_key:\n\u001B[32m     96\u001B[39m     msg += \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mfull_key: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfull_key\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m97\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m InstantiationException(msg) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n",
      "\u001B[31mInstantiationException\u001B[39m: Error in call to target 'openretina.data_io.sridhar_2025.dataloaders.white_noise_loader':\nTypeError(\"'int' object is not subscriptable\")\nfull_key: dataloader"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "c290dd72c61a5fa9",
   "metadata": {},
   "source": [
    "print(retina_indices)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b5bba4c5ac25668",
   "metadata": {},
   "source": [
    "input_shape = next(iter(dataloaders[\"train\"][retina_indices[0]]))\n",
    "cfg.model[\"in_shape\"] = list(input_shape[0].shape[1:])\n",
    "cfg.model[\"n_neurons_dict\"] = n_neurons_dict\n",
    "cfg.model[\"core\"][\"channels\"] = [input_shape[0].shape[1]] + cfg.model.hidden_channels\n",
    "cfg.model[\"n_neurons_dict\"] = n_neurons_dict\n",
    "cfg.model[\"readout\"][\"in_shape\"] = get_core_output_based_on_dimensions(cfg.model)\n",
    "print(cfg.model.in_shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4f6682d5",
   "metadata": {},
   "source": [
    "cfg.model[\"in_shape\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bf0cf2d73cf8ff53",
   "metadata": {},
   "source": [
    "batch = next(iter(dataloaders[\"train\"][retina_indices[0]]))\n",
    "print(\"input img shape\", batch[0].shape)\n",
    "print(\"response shape\", batch[1].shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "42cf81b8be2d757b",
   "metadata": {},
   "source": [
    "model = UnifiedCoreReadout(**cfg.model)\n",
    "model = model.float()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d78a47769788890c",
   "metadata": {},
   "source": [
    "log_save_path = os.path.join(cfg.paths.log_dir, \"cnns_wn/\")\n",
    "os.makedirs(\n",
    "    log_save_path,\n",
    "    exist_ok=True,\n",
    ")\n",
    "\n",
    "logger = lightning.pytorch.loggers.WandbLogger(\n",
    "    name=\"\",\n",
    "    save_dir=log_save_path,\n",
    ")\n",
    "early_stopping = lightning.pytorch.callbacks.EarlyStopping(\n",
    "    monitor=\"val_correlation\",\n",
    "    patience=10,\n",
    "    mode=\"max\",\n",
    "    verbose=False,\n",
    "    min_delta=0.001,\n",
    ")\n",
    "\n",
    "lr_monitor = lightning.pytorch.callbacks.LearningRateMonitor(logging_interval=\"epoch\")\n",
    "\n",
    "model_checkpoint = lightning.pytorch.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_correlation\", mode=\"max\", save_weights_only=False\n",
    ")\n",
    "\n",
    "trainer = lightning.Trainer(\n",
    "    max_epochs=100,\n",
    "    logger=None,\n",
    "    callbacks=[early_stopping, lr_monitor, model_checkpoint],\n",
    "    accelerator=\"gpu\",\n",
    "    log_every_n_steps=10,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e64cccf473ef6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T21:53:27.773230Z",
     "start_time": "2025-11-18T21:53:27.771264Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = LongCycler(dataloaders[\"train\"])\n",
    "val_loader = LongCycler(dataloaders[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abc30b9025c246c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T22:24:23.054017876Z",
     "start_time": "2025-11-18T21:53:29.106757Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201c7953a902c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
