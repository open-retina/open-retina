{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a gentle introduction to `openretina` through some visualization examples. No pre-requisites are needed to run this notebook, apart from having installed the package using one of the following options.\n",
    "\n",
    "Recommended:\n",
    "```\n",
    "git clone git@github.com:open-retina/open-retina.git\n",
    "cd open-retina\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "Alternative:\n",
    "\n",
    "```\n",
    "pip install openretina\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from IPython.display import clear_output, display\n",
    "from moviepy import VideoFileClip\n",
    "\n",
    "from openretina.data_io.hoefling_2024.constants import BADEN_TYPE_BOUNDARIES, RGC_GROUP_GROUP_ID_TO_CLASS_NAME\n",
    "from openretina.data_io.hoefling_2024.stimuli import movies_from_pickle\n",
    "from openretina.models.core_readout import load_core_readout_from_remote\n",
    "from openretina.utils.file_utils import get_cache_directory, get_local_file_path, optionally_download_from_url\n",
    "from openretina.utils.misc import CustomPrettyPrinter\n",
    "from openretina.utils.plotting import (\n",
    "    create_roi_animation,\n",
    "    display_video,\n",
    "    numpy_to_mp4_video,\n",
    "    prepare_video_for_display,\n",
    "    stitch_videos,\n",
    ")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")  # to display logs in jupyter notebooks\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "pp = CustomPrettyPrinter(indent=4, max_lines=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we are going to visualize the stimuli and model predictions for a simple \"Core + Readout\" model trained on data from Hoefling et al., 2024: [\"A chromatic feature detector in the retina signals visual context changes\"](https://elifesciences.org/articles/86860).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will set the directory to which the stimuli data are downloaded to. The default download target path for many functions within `openretina` is `OPENRETINA_CACHE_DIRECTORY`, which can be changed via its environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default directory for downloads will be ~/openretina_cache\n",
    "# To change this, uncomment the following line and change its path\n",
    "# os.environ[\"OPENRETINA_CACHE_DIRECTORY\"] = \"/Data/\"\n",
    "\n",
    "# You can then check if that directory has been correctly set by running:\n",
    "get_cache_directory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following download can take a while:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_stimulus_path = get_local_file_path(\n",
    "    \"https://huggingface.co/datasets/open-retina/open-retina/blob/main/euler_lab/hoefling_2024/stimuli/rgc_natstim_72x64_joint_normalized_2024-10-11.zip\"\n",
    ")\n",
    "\n",
    "movie_stimuli = movies_from_pickle(movie_stimulus_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the test movie, which we are going to use in our visualizations in the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_to_mp4_video(movie_stimuli.test, fps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The video stimuli used are crops of natural movies recorded in the green and UV channel from a \"mouse cam\" (see [Qiu et al., 2021](https://www.sciencedirect.com/science/article/pii/S096098222100676X))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to load a retina model that was trained on neural responses to this data. This is as easy as running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_core_readout_from_remote(\n",
    "    \"hoefling_2024_base_high_res\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now visualize the responses that the model gives to the video data. To do so we need to specify which recording session we want to predict. Here we pick by default the first one in the readout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, put the stimuli in a torch tensor, which is what the model expects.\n",
    "stim = torch.Tensor(movie_stimuli.test).to(model.device)\n",
    "\n",
    "# Second, we need to select one of the many experimental sessions the model was trained on to visualize a response.\n",
    "example_session = model.readout.sessions[0]  # Can pick any number as long as it is in range\n",
    "\n",
    "with torch.no_grad():\n",
    "    predicted_response = model.forward(stim.unsqueeze(0), data_key=example_session)\n",
    "predicted_response_numpy = predicted_response.squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the predicted response of example neurons with an interactive plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dropdown for neuron selection\n",
    "neuron_selector = widgets.Dropdown(\n",
    "    options=list(range(predicted_response_numpy.shape[1])),\n",
    "    value=0,\n",
    "    description=\"Neuron:\",\n",
    ")\n",
    "\n",
    "\n",
    "# Define the plotting function\n",
    "def plot_response(neuron_idx):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(predicted_response_numpy[:, neuron_idx])\n",
    "    plt.xlabel(\"Time [frames]\")\n",
    "    plt.ylabel(\"Response [a.u.]\")\n",
    "    sns.despine()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create an interactive widget\n",
    "widgets.interactive(plot_response, neuron_idx=neuron_selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things more interesting, we can also plot this from an \"ROI view\".\n",
    "ROI stands for Region Of Interest, and in our case, an ROI represents a retinal neuron that was imaged and segmented during data collection. Each ROI corresponds to a spatially localized neuron whose activity was recorded over time using 2P Calcium Imaging.\n",
    "\n",
    "To extract the ROI mask, we access the `data_info` field within the model, which is a dictionary containing various kinds of information about the data that was used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.data_info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what extra information we have about the sessions.\n",
    "pp.pprint(model.data_info[\"sessions_kwargs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we need is the roi_mask, and the roi_ids. Optionally we can pass also the cell type identity.\n",
    "\n",
    "roi_mask = model.data_info[\"sessions_kwargs\"][example_session][\"roi_mask\"]\n",
    "roi_ids = model.data_info[\"sessions_kwargs\"][example_session][\"roi_ids\"]\n",
    "cell_types = model.data_info[\"sessions_kwargs\"][example_session][\"group_assignment\"]\n",
    "\n",
    "roi_animation = create_roi_animation(\n",
    "    roi_mask=roi_mask, activity=predicted_response_numpy.T, roi_ids=roi_ids, max_activity=5, visualize_ids=True\n",
    ")\n",
    "numpy_to_mp4_video(roi_animation, fps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also pass cell type information to visualize the cells colour-coded by their type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = create_roi_animation(\n",
    "    roi_mask=roi_mask,\n",
    "    activity=predicted_response_numpy.T,\n",
    "    roi_ids=roi_ids,\n",
    "    cell_types=cell_types,  # array of cell type IDs\n",
    "    type_boundaries=BADEN_TYPE_BOUNDARIES,\n",
    "    max_activity=5,\n",
    "    visualize_ids=False,\n",
    ")\n",
    "\n",
    "numpy_to_mp4_video(video, fps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play with model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have gone through some basics, let's play with a more engaging example. \n",
    "\n",
    "We will take the ROI response view from above a step further, by:\n",
    "1. Showing the stimulus and the response side by side.\n",
    "2. Add the ability to visualize different \"broad\" cell types as defined in [Baden et al., 2016](https://www.nature.com/articles/nature16468) (Slow On, Fast On, Off, On-Off, uncertain RGCs, ACs).\n",
    "\n",
    "Do not worry about the visualisation code too much. To change which session's activity is visualized, you can change the example session in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the folder where the videos will be saved. This can be deleted later.\n",
    "videos_cache_folder = Path(get_cache_directory()).joinpath(\"./videos_cache\").resolve()\n",
    "videos_cache_folder.mkdir(exist_ok=True)\n",
    "print(f\"Videos will be saved in {videos_cache_folder}\")\n",
    "\n",
    "example_session = model.readout.sessions[2]  # Can pick any number as long as it is in range\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    predicted_response = model.forward(stim.unsqueeze(0), data_key=example_session)\n",
    "predicted_response_numpy = predicted_response.squeeze().cpu().numpy()\n",
    "\n",
    "# Extract metadata again\n",
    "roi_mask = model.data_info[\"sessions_kwargs\"][example_session][\"roi_mask\"]\n",
    "roi_ids = model.data_info[\"sessions_kwargs\"][example_session][\"roi_ids\"]\n",
    "cell_types = model.data_info[\"sessions_kwargs\"][example_session][\"group_assignment\"]\n",
    "\n",
    "# Get cell-type groups.\n",
    "baden_groups = np.array([RGC_GROUP_GROUP_ID_TO_CLASS_NAME[cell_type] for cell_type in cell_types])\n",
    "baden_unique_groups = np.unique(baden_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time a new type group is selected the activity video will be rendered, which might take around 40-50s. After the first display, it will be saved in `videos_cache_folder` and it will be shown again much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_layout = widgets.Layout(\n",
    "    display=\"flex\",\n",
    "    flex_flow=\"column\",\n",
    "    border=\"solid\",\n",
    "    width=\"100%\",\n",
    "    align_items=\"center\",\n",
    "    justify_content=\"center\",\n",
    ")\n",
    "style = {\"description_width\": \"initial\"}\n",
    "\n",
    "video_dict = {\n",
    "    group: mask for group, mask in zip(baden_unique_groups, [baden_groups == group for group in baden_unique_groups])\n",
    "}\n",
    "\n",
    "video_dict[\"All cell types\"] = np.ones_like(baden_groups).astype(bool)\n",
    "\n",
    "video_dropdown = widgets.Dropdown(\n",
    "    options=list(video_dict.keys()),\n",
    "    value=\"All cell types\",\n",
    "    description=\"Select Video: \",\n",
    "    layout=widgets.Layout(width=\"100%\", max_width=\"600px\", min_width=\"300px\"),\n",
    "    style=style,\n",
    ")\n",
    "\n",
    "video_output = widgets.Output()\n",
    "\n",
    "loading = widgets.Label(value=\"🔄 Loading...\", layout=widgets.Layout(visibility=\"hidden\"))\n",
    "empty = widgets.Label(value=\"\")\n",
    "\n",
    "\n",
    "def on_video_change(change):\n",
    "    \"\"\"Callback for dropdown selection change.\"\"\"\n",
    "\n",
    "    loading.layout.visibility = \"visible\"\n",
    "    with video_output:\n",
    "        clear_output(wait=True)\n",
    "        video_save_path = os.path.join(videos_cache_folder, f\"{example_session} {change['new']}.mp4\")\n",
    "        if os.path.exists(video_save_path):\n",
    "            display_video(video_array=None, video_save_path=video_save_path)\n",
    "        else:\n",
    "            group_mask = video_dict[change[\"new\"]]\n",
    "            stim_video = prepare_video_for_display(\n",
    "                movie_stimuli.test[:, 30:, ...]\n",
    "            )  # Skip the first 30 frames, to match response length\n",
    "            response_video = create_roi_animation(\n",
    "                roi_mask=roi_mask,\n",
    "                activity=predicted_response_numpy.T[group_mask],\n",
    "                roi_ids=roi_ids[group_mask],\n",
    "                cell_types=cell_types[group_mask],  # array of cell type IDs\n",
    "                type_boundaries=BADEN_TYPE_BOUNDARIES,  # boundaries between broad types\n",
    "                max_activity=5,\n",
    "                visualize_ids=False,\n",
    "            )\n",
    "\n",
    "            type_video = stitch_videos(stim_video, response_video)\n",
    "\n",
    "            # Before displaying, clear video area of all previous content\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            display_video(type_video, video_save_path=video_save_path, fps=30)\n",
    "\n",
    "    loading.layout.visibility = \"hidden\"\n",
    "\n",
    "\n",
    "# Attach the callback to the dropdown\n",
    "video_dropdown.observe(on_video_change, names=\"value\")\n",
    "\n",
    "# Display the widgets\n",
    "display(widgets.VBox([video_dropdown, loading, video_output, empty], layout=box_layout))\n",
    "\n",
    "# Initial video display\n",
    "on_video_change({\"new\": video_dropdown.value, \"old\": None, \"owner\": video_dropdown, \"type\": \"change\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un-scientific bonus: Showing any video to a retina model\n",
    "\n",
    "This final section is intended as a more exploratory and fun visualization feature than a rigorous analysis. \n",
    "\n",
    "While the model can generate predicted retinal responses to any video, these should be interpreted with caution. The model in question was trained on UV/green videos captured with a specialized camera, whereas arbitrary videos are typically in RGB and recorded under more diverse and less controlled conditions. This creates two potential sources of distribution shift: differences in spectral content and overall image statistics. While the model will still produce responses (if we manipulate the input videos to roughly match the ones it was trained on), they may not accurately reflect real retinal activity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cute_dog_video_path = optionally_download_from_url(\n",
    "    \"https://videos.pexels.com\", \"video-files/4411457/4411457-hd_1920_1080_25fps.mp4\", cache_folder=videos_cache_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_video(video_array=None, video_save_path=cute_dog_video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To \"show\" an arbitrary video to our retina model, we need to make sure the input size and statistics match the ones used to train the model. Let's fetch them first from \"data_info\", and then use them to rescale and normalize this video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.data_info[\"input_shape\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels, target_height, target_width = model.data_info[\"input_shape\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use moviepy to load the video into a numpy array\n",
    "clip_object = VideoFileClip(cute_dog_video_path)\n",
    "dog_clip_array = np.array(list(clip_object.iter_frames()))\n",
    "dog_clip_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "resized_dog_clip = np.stack(\n",
    "    [\n",
    "        cv2.resize(\n",
    "            frame,\n",
    "            (target_width, target_height),\n",
    "            interpolation=cv2.INTER_CUBIC,\n",
    "        )\n",
    "        for frame in dog_clip_array\n",
    "    ],\n",
    "    axis=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_dog_clip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_to_mp4_video(resized_dog_clip, fps=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the video is the appropriate size, we still need to do two things: have it in two channels (as the mouse retina model we exported was trained on videos on the UV and green channels), and normalize the input range for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.data_info[\"movie_norm_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a dummy UV channel by averaging the red and blue channels.\n",
    "dog_clip_two_channels = np.stack(\n",
    "    [\n",
    "        resized_dog_clip[:, :, :, 1],\n",
    "        (0.5 * resized_dog_clip[:, :, :, 0] + 0.5 * resized_dog_clip[:, :, :, 2]),\n",
    "    ],\n",
    "    axis=-1,\n",
    ")\n",
    "\n",
    "dog_clip_normalised = (\n",
    "    dog_clip_two_channels - model.data_info[\"movie_norm_dict\"][\"default\"][\"norm_mean\"]\n",
    ") / model.data_info[\"movie_norm_dict\"][\"default\"][\"norm_std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_to_mp4_video(dog_clip_normalised, fps=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to show the video to our retina model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "example_session = model.readout.sessions[2]\n",
    "\n",
    "# Put channel dimension first, as the model expects that.\n",
    "dog_clip_normalised = rearrange(dog_clip_normalised, \"t h w c -> c t h w\")\n",
    "dog_clip_tensor = torch.Tensor(dog_clip_normalised).to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predicted_dog_response = model.forward(dog_clip_tensor.unsqueeze(0), data_key=example_session)\n",
    "predicted_dog_response_numpy = predicted_dog_response.squeeze().cpu().numpy()\n",
    "\n",
    "# Extract metadata again\n",
    "roi_mask = model.data_info[\"sessions_kwargs\"][example_session][\"roi_mask\"]\n",
    "roi_ids = model.data_info[\"sessions_kwargs\"][example_session][\"roi_ids\"]\n",
    "cell_types = model.data_info[\"sessions_kwargs\"][example_session][\"group_assignment\"]\n",
    "\n",
    "stim_video = prepare_video_for_display(dog_clip_normalised[:, 30:, ...])\n",
    "\n",
    "response_video = create_roi_animation(\n",
    "    roi_mask=roi_mask,\n",
    "    activity=predicted_dog_response_numpy.T,\n",
    "    roi_ids=roi_ids,\n",
    "    cell_types=cell_types,\n",
    "    type_boundaries=BADEN_TYPE_BOUNDARIES,\n",
    "    max_activity=5,\n",
    "    visualize_ids=False,\n",
    ")\n",
    "\n",
    "type_video = stitch_videos(stim_video, response_video)\n",
    "\n",
    "numpy_to_mp4_video(type_video, fps=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is a wrap! We hope this notebook gave you some ideas on how to use a pre-trained retina model. To get a more in-depth view at training and other analyses possible within `openretina`, have a look at the other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, delete the video cache folder once you are done, to free up space.\n",
    "# import shutil\n",
    "\n",
    "# shutil.rmtree(videos_cache_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
