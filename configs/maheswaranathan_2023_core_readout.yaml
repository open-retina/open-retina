defaults:
  - data_io: maheswaranathan_2023
  - dataloader: maheswaranathan_2023
  - model: base_core_readout
  - training_callbacks:
    - early_stopping
    - lr_monitor
    - model_checkpoint
  - logger:
    - tensorboard
    - csv
  - trainer: default_deterministic
  - hydra: default
  - _self_ # values in this config will overwrite the defaults

exp_name: core_readout_maheswaranathan
seed: 42
check_stimuli_responses_match: false

paths:
  cache_dir: ${oc.env:OPENRETINA_CACHE_DIRECTORY} # Remote files are downloaded to this location
  # If data_dir is a local path, data will be read from there. If a remote link, the target will be downloaded to cache_dir.
  data_dir: https://huggingface.co/datasets/open-retina/open-retina/resolve/main/baccus_lab/maheswaranathan_2023/neural_code_data.zip
  log_dir: "." # Used as parent for output_dir. Will store train logs.
  output_dir: ${hydra:runtime.output_dir} # Modify in the "hydra/default.yaml" config

# Overwrite model defaults with specifics for Maheswaranathan 2023 input data format.
model:
  in_shape: [1, 100, 50, 50]
  hidden_channels: [16, 32]
  core:
    spatial_kernel_sizes: [15, 11] # as in the model from the paper. They used stacked convolutions though.

matmul_precision:
  _target_: torch.set_float32_matmul_precision
  precision: highest

trainer:
  gradient_clip_val: 1

