defaults:
  - data_io: maheswaranathan_2023
  - dataloader: maheswaranathan_2023
  - model: base_core_readout
  - training_callbacks:
    - early_stopping
    - lr_monitor
    - model_checkpoint
  - logger:
    - tensorboard
    - csv
  - trainer: default_deterministic
  - hydra: default
  - _self_ # values in this config will overwrite the defaults

exp_name: core_readout_maheswaranathan
seed: 42
check_stimuli_responses_match: false

data:
  root_dir: null # Choose the location of the root directory for the project
  data_dir: https://huggingface.co/datasets/open-retina/open-retina/resolve/main/baccus_lab/maheswaranathan_2023/neural_code_data.zip
  output_dir: ${data.root_dir}/openretina_assets/runs

# Overwrite model defaults with specifics for Maheswaranathan 2023 input data format.
model:
  in_shape: [1, 100, 50, 50]
  hidden_channels: [16, 32]
  spatial_kernel_sizes: [15, 11] # as in the model from the paper. They used stacked convolutions though.

matmul_precision:
  _target_: torch.set_float32_matmul_precision
  precision: highest

trainer:
  gradient_clip_val: 1

