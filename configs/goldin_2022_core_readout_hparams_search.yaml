defaults:
  - data_io: goldin_2022
  - dataloader: goldin_2022
  - model: core_klindt_readout
  - training_callbacks:
    - early_stopping
    - lr_monitor
    - model_checkpoint
    - weight_visualization
  - logger:
    - tensorboard
    - csv
    - mlflow
  - trainer: default_deterministic
  - hydra: default
  - override hydra/sweeper: optuna
  - override hydra/sweeper/sampler: tpe
  - _self_ # values in this config will overwrite the defaults

exp_name: core_readout_goldin_2022
seed: 42
check_stimuli_responses_match: false

paths:
  cache_dir: ${oc.env:OPENRETINA_CACHE_DIRECTORY} # Remote files are downloaded to this location
  # If data_dir is a local path, data will be read from there. If a remote link, the target will be downloaded to cache_dir.
  data_dir: 'https://huggingface.co/datasets/open-retina/open-retina/tree/main/marre_lab/goldin_2022/'
  log_dir: "." # Used as parent for output_dir. Will store train logs.
  output_dir: ${hydra:runtime.output_dir} # Modify in the "hydra/default.yaml" config

data_io:
  stimuli:
    specie: 'mouse'
  responses:
    specie: 'mouse'

objective_target: val_loss

hydra:
  run:
    dir: ${paths.log_dir}/openretina_assets/runs/${exp_name}/${now:%Y-%m-%d_%H-%M-%S}
  sweep:
    dir: ${paths.log_dir}/openretina_assets/runs/${exp_name}/${now:%Y-%m-%d_%H-%M-%S}
  sweeper:
    sampler:
      seed: 42
    direction: minimize
    study_name: ${exp_name}
    storage: null
    n_trials: 50
    n_jobs: 1
    params:
      model.mask_l1_reg : interval(1e-3, 10)
      model.core_gamma_input: interval(1e-3, 10)
      model.weights_l1_reg: interval(1e-3, 10)
      model.laplace_mask_reg: interval(1e-3, 10)
      model.spatial_kernel_sizes: choice([20],[25],[30])

# Overwrite model defaults
model:
  in_shape: [1, 1, 108, 108]
  hidden_channels: [4]
  temporal_kernel_sizes: [1]
  spatial_kernel_sizes: [30]
  core_gamma_temporal: 0.0
  core_gamma_input: 0.4654576767872573
  convolution_type: "time_independent"
  mask_l1_reg : 5.924553274051563
  weights_l1_reg : 6.075840974162483
  laplace_mask_reg: 1.706070712749228

matmul_precision:
  _target_: torch.set_float32_matmul_precision
  precision: highest

trainer:
  gradient_clip_val: 1
  log_every_n_steps: 10
  max_epochs: 100
  accumulate_grad_batches: 10
  precision: 16-mixed

training_callbacks:
  early_stopping:
    patience: 10
    monitor: "val_loss"
    mode: "min"
  model_checkpoint:
    monitor: "val_loss"
    mode: "min"
