# Adam optimizer configuration
_target_: torch.optim.Adam
lr: ${model.learning_rate}
betas: [0.9, 0.999]
eps: 1.0e-8
weight_decay: 0.0
amsgrad: false
