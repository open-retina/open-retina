# AdamW optimizer configuration
# This is the default optimizer used in the codebase
_target_: torch.optim.AdamW
lr: ${model.learning_rate}
betas: [0.9, 0.999]
eps: 1.0e-8
weight_decay: 0.01
amsgrad: false
